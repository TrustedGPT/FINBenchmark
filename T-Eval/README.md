## ä»‹ç»
è¿™æ˜¯åœ¨Tevalçš„ä»£ç æ¡†æ¶çš„åŸºç¡€ä¸Šæ„å»ºçš„é‡‘èæ™ºèƒ½ä½“benchmark. æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨ä»¥ä¸‹å…­ä¸ªç»´åº¦çš„èƒ½åŠ›: instruct, plan, reason, retrieve, understand ä»¥åŠ review.
## ğŸ› ï¸ å®‰è£…

```bash
$ cd T-Eval
$ pip install -r requirements.txt
$ cd lagent && pip install -e .
```

##  ğŸ›«ï¸ å¼€å§‹

### ğŸ¤– API Models

1. è®¾ç½®OPENAI_API_KEYå’ŒOPENAI_API_BASE
```bash
export OPENAI_API_KEY=xxxxxxxxx
export OPENAI_API_BASE=xxxxxxxxx
```
2. ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿è¡Œè¯„æµ‹(model_name å¯ä»¥æ˜¯OpenAIæ¨¡å‹æˆ–è€…æ”¯æŒopenaiåº“è°ƒç”¨çš„æ¨¡å‹ï¼Œä¾‹å¦‚deepseek-chat)
```bash
sh test.sh model_name output_path
```

### ğŸ¤— Local Models

1.ä½¿ç”¨vllméƒ¨ç½²ä½ çš„æ¨¡å‹
```bash
CUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server \
    --model model_path \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.9 \
    --served-model-name model_name \
    --block-size 16  \
    --trust-remote-code \
    --chat-template chat_templates_path \
    --port 8081
```
2.å¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œå¯ä»¥ä»T-Eval/chat_templatesç›®å½•ä¸‹é€‰æ‹©ç›¸åº”çš„æ¨¡æ¿æ–‡ä»¶ï¼›ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿è¡Œè¯„æµ‹
```bash
export MKL_THREADING_LAYER=GNU \
export MKL_SERVICE_FORCE_INTEL=1 \
export OPENAI_API_KEY="EMPTY" \
export OPENAI_API_BASE=http://0.0.0.0:8081/v1

sh test.sh model_name output_path
```

### ğŸ’« æœ€ç»ˆç»“æœ
ä¸€æ—¦ä½ æµ‹è¯•å®Œäº†æ‰€æœ‰çš„æ•°æ®ï¼Œç»“æœçš„ç»†èŠ‚ä¼šæ”¾åœ¨ `output_path/teval_output/result.json`
